{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## Working with an LLM programmatically\n",
    "\n",
    "ChatGPT のような大規模言語モデル (LLM) をこれまでに操作したことがあるはずです。これは通常、UI またはアプリケーションを介して行われます。\n",
    "\n",
    "このノートブックでは、Python を使用して、LLM の API を介して直接接続し、クエリを実行します。このラボでは、モデル **Granite-7B-Instruct** (https://huggingface.co/ibm-granite/granite-7b-instruct) を選択しました。これは、IBM Research によって開発された完全にオープン ソースのモデル (Apache 2.0 ライセンス) です。\n",
    "\n",
    "このモデルは、小さいモデルであっても、実行するには 24 GB の RAM を備えた GPU が必要なため、ラボ クラスターにすでにデプロイされています..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### 要件とインポート\n",
    "\n",
    "ラボの指示に従って起動する適切なワークベンチ イメージを選択した場合は、必要なライブラリがすべてすでに用意されているはずです。そうでない場合は、次のセルの最初の行のコメントを解除して、適切なパッケージをすべてインストールします。その後、必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 適切なワークベンチ イメージを選択していない場合、またはこのノートブックをワークショップ環境外で使用している場合にのみ、次の行のコメントを解除します。\n",
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import VLLMOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "\n",
    "Langchain (https://www.langchain.com/) は、言語モデルを利用したアプリケーションを開発するためのフレームワークです。LLM API を適切にクエリするために手動で記述する必要があるすべての定型コードを処理します。\n",
    "\n",
    "まず、LLM API をクエリできる場所とモデルに適用されるいくつかのパラメータで定義される **llm** インスタンスを作成します。たとえば、`max_new_tokens` は、モデルに最大 512 個のトークン (単語または単語の一部) で応答するように指示します。ここで非常に低く設定されている `temperature` は、モデルに真実に基づいたままで、あまり「創造的」にならないように指示します。結局のところ、ここでは派手な詩を書こうとしているわけではありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Inference Server URL\n",
    "inference_server_url = \"http://granite-7b-instruct-predictor.ic-shared-llm.svc.cluster.local:8080\"\n",
    "\n",
    "# LLM definition\n",
    "llm = VLLMOpenAI(           # 私たちは vLLM OpenAI 互換 API クライアントを使用しています。ただし、モデルは OpenAI ではなく OpenShift AI 上で実行されています。\n",
    "    openai_api_key=\"EMPTY\",   # そのため、これには OpenAI キーは必要ありません。\n",
    "    openai_api_base= f\"{inference_server_url}/v1\",\n",
    "    model_name=\"granite-7b-instruct\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "また、モデルに送信するすべてのリクエスト (「プロンプト」) に適用される **template** も必要です。\n",
    "\n",
    "モデルにクエリを実行する場合、ユーザーが入力したものを直接送信することはほとんど望ましくありません。このエントリに加えて、モデルが処理方法を理解できるように、適切な指示をモデルに与える必要があります。つまり、何にどのように答えるか、何に答えてはいけないか、どのような口調で答えるかなどです..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<|system|>\n",
    "You are a helpful, respectful and honest assistant. Always be as helpful as possible, while being safe.\n",
    "You will be asked a question, to which you must give an answer.\n",
    "Your answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, answer \"I don't know\".\n",
    "\n",
    "<|user|>\n",
    "### QUESTION:\n",
    "{input}\n",
    "\n",
    "### ANSWER:\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "Langchain を使用すると、これらの要素を簡単に「つなぎ合わせて」、モデルのクエリに使用する **conversation** オブジェクトを作成できるようになりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "これでモデルをクエリする準備ができました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca714bca-7cec-4afc-b275-fa389c05a993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is Artificial Intelligence?\"\n",
    "\n",
    "conversation.invoke(input=query); # 行末の\";\"は最終出力（ストリームされた回答の繰り返し）を非表示にします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0089476-bba0-4093-8be8-1469780afaba",
   "metadata": {},
   "source": [
    "必要に応じて、セクション 3.7 のこのノートブックに戻って、オプションの演習を行うこともできます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
