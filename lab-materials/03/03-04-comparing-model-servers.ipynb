{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## Comparing models for our different tasks\n",
    "\n",
    "このノートブックでは、別のモデルである Flan-T5-large を Granite-7B-Instruct と並行して使用し、その動作を確認します。\n",
    "\n",
    "Flan-T5-Large は確かに小さく、GPU なしで実行され、4 GB の RAM しか使用しませんが、タスクに十分対応できるでしょうか?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### 要件とインポート\n",
    "\n",
    "ラボの指示に従って起動する適切なワークベンチ イメージを選択した場合は、必要なライブラリがすべてすでに用意されているはずです。そうでない場合は、次のセルの最初の行のコメントを解除して、適切なパッケージをすべてインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 適切なワークベンチ イメージを選択していない場合、またはこのノートブックをワークショップ環境外で使用している場合にのみ、次の行のコメントを解除します。\n",
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFaceTextGenInference, VLLMOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain パイプライン\n",
    "\n",
    "ここでも、Langchain を使用して要約パイプラインを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Inference Server URL\n",
    "inference_server_url = \"http://granite-7b-instruct-predictor.ic-shared-llm.svc.cluster.local:8080\"\n",
    "\n",
    "# LLM definition\n",
    "llm = VLLMOpenAI(           # 私たちは vLLM OpenAI 互換 API クライアントを使用しています。ただし、モデルは OpenAI ではなく OpenShift AI 上で実行されています。\n",
    "    openai_api_key=\"EMPTY\",   # そのため、これには OpenAI キーは必要ありません。\n",
    "    openai_api_base= f\"{inference_server_url}/v1\",\n",
    "    model_name=\"granite-7b-instruct\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782046b7-f0a4-487c-86fc-3131e668c6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Flan-T5-Large LLM Inference Server URL\n",
    "inference_server_url_flan_t5 = \"http://llm-flant5.ic-shared-llm.svc.cluster.local:3000/\"\n",
    "\n",
    "# LLM definition\n",
    "llm_flant5 = HuggingFaceTextGenInference(\n",
    "    inference_server_url=inference_server_url_flan_t5,\n",
    "    max_new_tokens=96,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    " **template** は両方のモデルで同じになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<|system|>\n",
    "You are a helpful, respectful and honest assistant.\n",
    "Always assist with care, respect, and truth. Respond with utmost utility yet securely.\n",
    "Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\n",
    "I will give you a text, then ask a question about it. Give a precise and as concise as possible answer to this question.\n",
    "<|user|>\n",
    "### TEXT:\n",
    "{text}\n",
    "\n",
    "### QUESTION:\n",
    "{query}\n",
    "\n",
    "### ANSWER:\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "そして、2 つのモデルをクエリするために使用する 2 つの **会話** オブジェクトを作成できるようになりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = prompt | llm\n",
    "conversation_flant5 = prompt | llm_flant5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "これで、モデルをクエリする準備ができました。\n",
    "\n",
    "この例では、1 つのクレームのみをクエリして、何が起こるかを確認します。 もちろん、別のクレームでも自由に試してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac009d5-d558-4258-9735-4fb0de46c309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = 'claims/claim1.json'\n",
    "\n",
    "# JSONファイルを開く\n",
    "claims = {}\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "claims[filename] = data\n",
    "\n",
    "# クレーム内容とクエリのリスト\n",
    "text_input = f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\"\n",
    "sentiment_query = \"What is the sentiment of the person sending this claim?\"\n",
    "location_query = \"Where does the event the claim is related to happen?\"\n",
    "time_query = \"When does the event the claim is related to happen?\"\n",
    "\n",
    "# クレームの解析\n",
    "print(f\"***************************\")\n",
    "print(f\"* Claim: {filename}\")\n",
    "print(f\"***************************\")\n",
    "print(\"Original content:\")\n",
    "print(\"-----------------\")\n",
    "print(f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\\n\\n\")\n",
    "print('Analysis with Granite-7B-Instruct:')\n",
    "print(\"--------\")\n",
    "start_granite = time.time()\n",
    "print(f\"- Sentiment: \")\n",
    "conversation.invoke(input={\"text\": text_input, \"query\": sentiment_query});\n",
    "print(\"\\n- Location: \")\n",
    "conversation.invoke(input={\"text\": text_input, \"query\": location_query});\n",
    "print(\"\\n- Time: \")\n",
    "conversation.invoke(input={\"text\": text_input, \"query\": time_query});\n",
    "print(\"\\n\\n                          ----====----\\n\")\n",
    "end_granite = time.time()\n",
    "print('Analysis with Flan-T5-Large:')\n",
    "print(\"--------\")\n",
    "start_flan = time.time()\n",
    "print(f\"- Sentiment: \")\n",
    "conversation_flant5.invoke(input={\"text\": text_input, \"query\": sentiment_query});\n",
    "print(\"\\n- Location: \")\n",
    "conversation_flant5.invoke(input={\"text\": text_input, \"query\": location_query});\n",
    "print(\"\\n- Time: \")\n",
    "conversation_flant5.invoke(input={\"text\": text_input, \"query\": time_query});\n",
    "print(\"\\n\\n                          ----====----\\n\")\n",
    "end_flan = time.time()\n",
    "\n",
    "print(f\"Granite analysis time: {end_granite - start_granite:.2f} seconds\")\n",
    "print(f\"Flan analysis time: {end_flan - start_flan:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28a5b0-6c93-42ba-84dd-42e17746d11d",
   "metadata": {},
   "source": [
    "ご覧のとおり、Flan-T5-Large は 7 億 7000 万のパラメータ モデルのみであるため、一部の結果を生成するのが速い可能性があります。ただし、それらの結果は精度や詳細度が低くなります。したがって、ある程度は機能しますが、結果は 70 億のパラメータである Granite-7B-Instruct の結果には遠く及びません。\n",
    "\n",
    "LLM を使用する際の秘訣は、必要なパフォーマンスと精度、およびそれに伴うコストとともに必要なリソースとの間で適切なバランスを見つけることです。\n",
    "\n",
    "したがって、データが変更されたりモデルが進化したりしても、常に期待どおりの動作が得られるように、信頼性チェックを実施することが重要です。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
