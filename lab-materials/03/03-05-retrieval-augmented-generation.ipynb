{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extending the capabilities of our model\n",
    "\n",
    "LLM は非常に有能なツールですが、トレーニングに使用された知識や情報の範囲内でしか機能しません。結局のところ、自分が知っていることしか知らないのです。しかし、トレーニング データに含まれていない質問をする必要がある場合はどうでしょうか。または、トレーニング データには含まれていないが、それに関連する質問をする必要がある場合はどうでしょうか。\n",
    "\n",
    "この問題を解決するには、所有しているリソースと、それに費やせる時間やお金に応じて、さまざまな方法があります。いくつかのオプションを以下に示します。\n",
    "\n",
    "- 必要な情報を含めるようにモデルを完全に再トレーニングします。LLM の場合、文字通り数千の GPU を数週間稼働させることができるのは、世界でもほんの一握りの企業だけです。\n",
    "- この新しい情報を使用してモデルを微調整します。これには必要なリソースが大幅に少なく、通常は数時間または数分で実行できます (モデルのサイズによって異なります)。ただし、モデルを完全に再トレーニングしないため、新しい情報が回答に完全に統合されない可能性があります。微調整は、特定のコンテキストや語彙をよりよく理解することに優れていますが、新しい知識を注入することにはそれほど優れていません。さらに、情報を追加したいときはいつでも、モデルを再トレーニングして再デプロイする必要があります。\n",
    "- この新しい情報をデータベースに格納し、クエリに関連する部分を取得して、このクエリにコンテキストとして追加してから、LLM に送信します。この手法は、**Retrieval Augmented Generation (RAG、検索拡張生成)** と呼ばれます。この新しい知識を活用するためにモデルを再トレーニングまたは微調整する必要がなく、いつでも簡単に更新できるという点が興味深いです。\n",
    "\n",
    "[Milvus](https://milvus.io/) を使用してベクター データベースを既に準備しており、そこに [カリフォルニア運転者ハンドブック](https://www.dmv.ca.gov/portal/handbook/california-driver-handbook/) のコンテンツが ([Embeddings](https://www.ibm.com/topics/embedding) の形式で) 保存されています。\n",
    "\n",
    "このノートブックでは、RAG を使用して **クレームに関するクエリをいくつか実行** し、LLM を変更せずにこの新しい知識がどのように役立つかを確認します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### 要件とインポート\n",
    "\n",
    "ラボの指示に従って起動する適切なワークベンチ イメージを選択した場合は、必要なライブラリがすべてすでに用意されているはずです。そうでない場合は、次のセルの最初の行のコメントを解除して、適切なパッケージをすべてインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 適切なワークベンチ イメージを選択していない場合、またはこのノートブックをワークショップ環境外で使用している場合にのみ、次の行のコメントを解除します。\n",
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt\n",
    "\n",
    "import json\n",
    "\n",
    "import transformers\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from milvus_retriever_with_score_threshold import \\\n",
    "    MilvusRetrieverWithScoreThreshold\n",
    "\n",
    "# Turn off warnings when downloading the embedding model\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain 要素\n",
    "\n",
    "ここでも、Langchain を使用してタスク パイプラインを定義します。\n",
    "\n",
    "まず、クエリを送信する **LLM** です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Inference Server URL\n",
    "inference_server_url = \"http://granite-7b-instruct-predictor.ic-shared-llm.svc.cluster.local:8080\"\n",
    "\n",
    "# LLM definition\n",
    "llm = VLLMOpenAI(           # 私たちは vLLM OpenAI 互換 API クライアントを使用しています。ただし、モデルは OpenAI ではなく OpenShift AI 上で実行されています。\n",
    "    openai_api_key=\"EMPTY\",   # そのため、これには OpenAI キーは必要ありません。\n",
    "    openai_api_base= f\"{inference_server_url}/v1\",\n",
    "    model_name=\"granite-7b-instruct\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa13907-14f1-4995-9756-8778c19a2101",
   "metadata": {},
   "source": [
    "次に、カリフォルニア運転者ハンドブックを準備して保存した**ベクター データベース**に接続します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849c1a0-7fe5-425f-853d-6a9e67a38971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# まず、ハンドブックを処理するために使用した埋め込みを定義します\n",
    "model_kwargs = {\"trust_remote_code\": True}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "            model_kwargs=model_kwargs,\n",
    "            show_progress=False,\n",
    "        )\n",
    "\n",
    "# 次に、Milvusベクトルストアから関連データを取得するリトリーバーを定義します。\n",
    "retriever = MilvusRetrieverWithScoreThreshold(\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=\"california_driver_handbook_1_0\",\n",
    "            collection_description=\"\",\n",
    "            collection_properties=None,\n",
    "            connection_args={\n",
    "                \"host\": \"vectordb-milvus.ic-shared-milvus.svc.cluster.local\",\n",
    "                \"port\": \"19530\",\n",
    "                \"user\": \"root\",\n",
    "                \"password\": \"Milvus\",\n",
    "            },\n",
    "            consistency_level=\"Session\",\n",
    "            search_params=None,\n",
    "            k=4,\n",
    "            score_threshold=0.99,\n",
    "            enable_dynamic_field=True,\n",
    "            text_field=\"page_content\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "ここで、クエリを作成するために使用する **template** を定義します。このテンプレートには **References** セクションが含まれていることに注意してください。ここに、ベクター データベースから返されたドキュメントが挿入されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<|system|>\n",
    "You are a helpful, respectful and honest assistant named \"Parasol Assistant\".\n",
    "You will be given a claim summary, references to provide you with information, and a question.\n",
    "You must answer the question based as much as possible on this claim with the help of the references.\n",
    "Always answer as helpfully as possible, while being safe.\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "<|user|>\n",
    "Claim Summary:\n",
    "{claim}\n",
    "\n",
    "References:\n",
    "{{context}}\n",
    "\n",
    "Question: {{question}}\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "これで、モデルをクエリする準備ができました!\n",
    "\n",
    "`claims` フォルダーには、受信できるクレームの例を含む JSON ファイルがあります。最初のクレームを読み取り、それに関連する質問をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca714bca-7cec-4afc-b275-fa389c05a993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# クレームを読み取り、その内容を\"claim\"変数に格納します\n",
    "\n",
    "filename = 'claims/claim1.json'\n",
    "\n",
    "# JSONファイルを開く\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "claim = data[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cde40-3571-4e3c-9b05-78389765c98f",
   "metadata": {},
   "source": [
    "### 最初のテスト、追加の知識なし\n",
    "\n",
    "クレームに関する最初のクエリから始めましょう。ただし、ベクター データベースの助けは借りません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac398b4-d555-45e5-aab9-d9b319f07108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# クエリを作成して送信します。\n",
    "\n",
    "query = \"Was Daniel allowed to pass at the red light?\"\n",
    "\n",
    "# 同じテンプレートを異なるタイプのクエリで再利用するためのクイック ハック。\n",
    "prompt_template = template.format(claim=claim)\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
    "conversation = prompt | llm\n",
    "\n",
    "resp = conversation.invoke(input={\"context\": \"\", \"question\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f714d-c6e7-4220-a16b-fc65dbae91fb",
   "metadata": {},
   "source": [
    "答えが有効であることがわかります。ここで、モデルは交通規制に関する一般的な理解を使用しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e9a93-9b81-424a-96a9-f447e417c8c1",
   "metadata": {},
   "source": [
    "### 2 回目のテスト、知識を追加\n",
    "\n",
    "同じプロンプトとクエリを使用しますが、今回はモデルがカリフォルニアのドライバー ハンドブックの参照資料にアクセスします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac009d5-d558-4258-9735-4fb0de46c309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# クエリを作成して送信します。\n",
    "\n",
    "query = \"Was Daniel allowed to pass at the red light?\"\n",
    "\n",
    "prompt_template = template.format(claim=claim)\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "            llm,\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "resp = rag_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5659f0-a27f-4b9e-8dd1-e05f37671c8f",
   "metadata": {},
   "source": [
    "非常にすばらしいですね。これで、モデルは遵守すべきルールをより正確に参照するようになりました。\n",
    "\n",
    "しかし、この情報はどこから得たのでしょうか? ベクター データベースから回答に関連付けられたソースを調べることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e424c52-9a1a-4425-a105-4e0744ec0da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_sources(input_list):\n",
    "    sources = \"\"\n",
    "    if len(input_list) != 0:\n",
    "        sources += input_list[0].metadata[\"metadata\"][\"source\"] + ', page: ' + str(input_list[0].metadata[\"metadata\"][\"page\"])\n",
    "        page_list = [input_list[0].metadata[\"metadata\"][\"page\"]]\n",
    "        for item in input_list:\n",
    "            if item.metadata[\"metadata\"][\"page\"] not in page_list: # 重複の回避\n",
    "                page_list.append(item.metadata[\"metadata\"][\"page\"])\n",
    "                sources += ', ' + str(item.metadata[\"metadata\"][\"page\"])\n",
    "    return sources\n",
    "\n",
    "\n",
    "results = format_sources(resp['source_documents'])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8cd32-0bdb-484d-a8bd-fb108ce2f131",
   "metadata": {},
   "source": [
    "これで完了です。これで、LLM を外部の知識で補完する方法がわかりました。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
